{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python tutorial for using Attentive Chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install kipoi\n",
    "This can be done by using `pip`. (Ex `pip install kipoi`). Note that you need anaconda or miniconda installed. Refer to https://kipoi.org/docs/#installation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Attentive Chrome in your Python program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import kipoi. Also, we download our example file for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kipoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we want to predict for cell type E005. We first go ahead and create a model for the cell type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://zenodo.org/api/files/2bf982b6-143f-49f6-b9ad-1b1e60f67292/E005_attchrome_avgAUC_model.pt?download=1 to .kipoi/models/AttentiveChrome/downloaded/model_files/E005/weights/19f61dca439ffcf7bbe44ca15238ff4d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "238kB [00:02, 117kB/s]                             \n",
      "miniconda3/envs/kipoi-AttentiveChrome/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = kipoi.get_model(\"AttentiveChrome/E005\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to predict using this model object. First is to predict using the pipeline. This makes prediction for all batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://zenodo.org/record/2640883/files/test.csv?download=1 to .kipoi/models/AttentiveChrome/downloaded/example_files/input_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24.6kB [00:02, 12.0kB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl_dictionary: {'input_file': '.kipoi/models/AttentiveChrome/downloaded/example_files/input_file'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genes: 10\n",
      "Number of entries: 1000\n",
      "Number of HMs: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dl_dictionary = model.default_dataloader.example_kwargs #This is an example dataloader.\n",
    "print(\"dl_dictionary:\", dl_dictionary)\n",
    "\n",
    "prediction = model.pipeline.predict(dl_dictionary)\n",
    "\n",
    "#If you wish to make prediction on your own dataset, run this code:\n",
    "#prediction = model.pipeline.predict({\"input_file\": \"path to input file\", \"bin_size\": {some integer}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output of prediction is a numpy array containing the output from the final softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction result:\n",
      "[[0.6430358 ]\n",
      " [0.04643877]\n",
      " [0.67054904]\n",
      " [0.2707719 ]\n",
      " [0.6830173 ]\n",
      " [0.55665994]\n",
      " [0.18436128]\n",
      " [0.13774122]\n",
      " [0.06227126]\n",
      " [0.60603684]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction result:\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following example doesn't work! The code is just kept as an example.**\n",
    "\n",
    "Another way to make a prediction is to predict for single batches. We first need to create our dataloader.\n",
    "Then, we can create an iterator of fixed batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genes: 6600\n",
      "Number of entries: 660000\n",
      "Number of HMs: 7\n"
     ]
    }
   ],
   "source": [
    "# dl = model.default_dataloader.init_example()\n",
    "# it = dl.batch_iter(batch_size=32) #iterator of batch size 32\n",
    "\n",
    "# single_batch = next(it) #this gets us a single batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sake of example, let's make a prediction on the first 10 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction on batch 0\n",
      "Making prediction on batch 1\n",
      "Making prediction on batch 2\n",
      "Making prediction on batch 3\n",
      "Making prediction on batch 4\n",
      "Making prediction on batch 5\n",
      "Making prediction on batch 6\n",
      "Making prediction on batch 7\n",
      "Making prediction on batch 8\n",
      "Making prediction on batch 9\n"
     ]
    }
   ],
   "source": [
    "# The batch var isn't defined so the code fails!\n",
    "# for i in range(10):\n",
    "#     print(\"Making prediction on batch\",i)\n",
    "#     prediction = model.predict_on_batch(batch['inputs'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6684ee16a3312f4b873aae1648d9ebbd237a1dd41ffc333dbf92e881f9e73800"
  },
  "kernelspec": {
   "display_name": "Python 3.6.3 ('kipoi-AttentiveChrome')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
